{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/SYSTRAN/faster-whisper\n",
        "\n",
        "https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2\n"
      ],
      "metadata": {
        "id": "Y2EwDqvPw78X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SetUp and Restart Session\n",
        "!pip install faster-whisper\n",
        "!pip install gradio\n",
        "# !rm -rf /content/faster-whisper-large-v3-turbo-ct2\n",
        "\n",
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2\n",
        "# !rm -rf /content/faster-whisper-large-v3-turbo-ct2\n",
        "# from huggingface_hub import snapshot_download\n",
        "\n",
        "# repo_id = \"deepdml/faster-whisper-large-v3-turbo-ct2\"\n",
        "# local_dir = \"faster-whisper-large-v3-turbo-ct2\"\n",
        "# snapshot_download(repo_id=repo_id, local_dir=local_dir, repo_type=\"model\")\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "def conditional_download(url, download_file_path, redownload=False):\n",
        "    print(f\"Downloading {os.path.basename(download_file_path)}\")\n",
        "    base_path = os.path.dirname(download_file_path)\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(base_path):\n",
        "        os.makedirs(base_path)\n",
        "\n",
        "    # Skip download if the file exists and redownload is False\n",
        "    if os.path.exists(download_file_path) and not redownload:\n",
        "        print(f\"File {download_file_path} already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    # If redownload is True, remove the existing file\n",
        "    if os.path.exists(download_file_path) and redownload:\n",
        "        os.remove(download_file_path)\n",
        "\n",
        "    # Try opening the URL and get the total file size\n",
        "    try:\n",
        "        request = urllib.request.urlopen(url)\n",
        "        total = int(request.headers.get('Content-Length', 0))\n",
        "    except urllib.error.URLError as e:\n",
        "        print(f\"Error: Unable to open the URL - {url}\")\n",
        "        print(f\"Reason: {e.reason}\")\n",
        "        return\n",
        "\n",
        "    # Start downloading with a progress bar\n",
        "    with tqdm(total=total, desc=f\"Downloading {os.path.basename(download_file_path)}\", unit='B', unit_scale=True, unit_divisor=1024) as progress:\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, download_file_path, reporthook=lambda count, block_size, total_size: progress.update(block_size))\n",
        "        except urllib.error.URLError as e:\n",
        "            print(f\"Error: Failed to download the file from the URL - {url}\")\n",
        "            print(f\"Reason: {e.reason}\")\n",
        "            return\n",
        "\n",
        "    print(f\"Download successful! Saved at: {download_file_path}\")\n",
        "\n",
        "\n",
        "# Step 3: Download the models\n",
        "def download_whisper_model(base_path, redownload=False):\n",
        "    print(\"Starting model downloads...\")\n",
        "    # https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2\n",
        "    model_urls = [\n",
        "        (\"https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2/resolve/main/config.json\", f\"{base_path}/faster-whisper-large-v3-turbo-ct2/config.json\"),\n",
        "        (\"https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2/resolve/main/model.bin\", f\"{base_path}/faster-whisper-large-v3-turbo-ct2/model.bin\"),\n",
        "        (\"https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2/resolve/main/preprocessor_config.json\", f\"{base_path}/faster-whisper-large-v3-turbo-ct2/preprocessor_config.json\"),\n",
        "        (\"https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2/resolve/main/tokenizer.json\", f\"{base_path}/faster-whisper-large-v3-turbo-ct2/tokenizer.json\"),\n",
        "        (\"https://huggingface.co/deepdml/faster-whisper-large-v3-turbo-ct2/resolve/main/vocabulary.json\", f\"{base_path}/faster-whisper-large-v3-turbo-ct2/vocabulary.json\"),\n",
        "    ]\n",
        "\n",
        "    for url, path in model_urls:\n",
        "        conditional_download(url, path, redownload=redownload)\n",
        "\n",
        "    print(\"All models downloaded successfully.\")\n",
        "\n",
        "# base_path = \".\"\n",
        "base_path = \"/content\"\n",
        "download_whisper_model(base_path, redownload=True)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b52qJClFUZJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After auto-restarting the session, run from the next cell."
      ],
      "metadata": {
        "id": "UzeNA6JJwYCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load faster-whisper\n",
        "import math\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "import subprocess\n",
        "from IPython.display import Audio\n",
        "from faster_whisper import WhisperModel\n",
        "import os\n",
        "import mimetypes\n",
        "import shutil\n",
        "import re\n",
        "import uuid\n",
        "\n",
        "def clean_file_name(file_path):\n",
        "    # Get the base file name and extension\n",
        "    file_name = os.path.basename(file_path)\n",
        "    file_name, file_extension = os.path.splitext(file_name)\n",
        "\n",
        "    # Replace non-alphanumeric characters with an underscore\n",
        "    cleaned = re.sub(r'[^a-zA-Z\\d]+', '_', file_name)\n",
        "\n",
        "    # Remove any multiple underscores\n",
        "    clean_file_name = re.sub(r'_+', '_', cleaned).strip('_')\n",
        "\n",
        "    # Generate a random UUID for uniqueness\n",
        "    random_uuid = uuid.uuid4().hex[:6]\n",
        "\n",
        "    # Combine cleaned file name with the original extension\n",
        "    clean_file_path = os.path.join(os.path.dirname(file_path), clean_file_name + f\"_{random_uuid}\" + file_extension)\n",
        "\n",
        "    return clean_file_path\n",
        "\n",
        "def get_audio_file(uploaded_file):\n",
        "    global base_path\n",
        "    # ,device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # Detect the file type (audio/video)\n",
        "    mime_type, _ = mimetypes.guess_type(uploaded_file)\n",
        "    # Create the folder path to store audio files\n",
        "    audio_folder = f\"{base_path}/subtitle_audio\"\n",
        "    os.makedirs(audio_folder, exist_ok=True)\n",
        "    # Initialize variable for the audio file path\n",
        "    audio_file_path = \"\"\n",
        "    if mime_type and mime_type.startswith('audio'):\n",
        "        # If it's an audio file, save it as is\n",
        "        audio_file_path = os.path.join(audio_folder, os.path.basename(uploaded_file))\n",
        "        audio_file_path=clean_file_name(audio_file_path)\n",
        "        shutil.copy(uploaded_file, audio_file_path)  # Move file to audio folder\n",
        "\n",
        "    elif mime_type and mime_type.startswith('video'):\n",
        "        # If it's a video file, extract the audio\n",
        "        audio_file_name = os.path.splitext(os.path.basename(uploaded_file))[0] + \".mp3\"\n",
        "        audio_file_path = os.path.join(audio_folder, audio_file_name)\n",
        "        audio_file_path=clean_file_name(audio_file_path)\n",
        "\n",
        "        # Extract the file extension from the uploaded file\n",
        "        file_extension = os.path.splitext(uploaded_file)[1]  # Includes the dot, e.g., '.mp4'\n",
        "\n",
        "        # Generate a random UUID and create a new file name with the same extension\n",
        "        random_uuid = uuid.uuid4().hex[:6]\n",
        "        new_file_name = random_uuid + file_extension\n",
        "\n",
        "        # Set the new file path in the subtitle_audio folder\n",
        "        new_file_path = os.path.join(audio_folder, new_file_name)\n",
        "\n",
        "        # Copy the original video file to the new location with the new name\n",
        "        shutil.copy(uploaded_file, new_file_path)\n",
        "        if device==\"cuda\":\n",
        "          command = f\"ffmpeg -hwaccel cuda -i {new_file_path} {audio_file_path} -y\"\n",
        "        else:\n",
        "          command = f\"ffmpeg -i {new_file_path} {audio_file_path} -y\"\n",
        "\n",
        "        # if device==\"cuda\":\n",
        "        #   command = f\"ffmpeg -hwaccel cuda -i {new_file_path} -vn -ab 320k -ar 48000 -c:a copy -y {audio_file_path}\"\n",
        "        # else:\n",
        "        #   command = f\"ffmpeg -i {new_file_path} -vn -ab 320k -ar 48000 -y {audio_file_path} -y\"\n",
        "\n",
        "        subprocess.run(command, shell=True)\n",
        "        if os.path.exists(new_file_path):\n",
        "          os.remove(new_file_path)\n",
        "    # Return the saved audio file path\n",
        "    return audio_file_path\n",
        "\n",
        "\n",
        "def is_gpu_memory_over_limit(limit_gb=14.5):\n",
        "    # Run nvidia-smi and capture the output\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
        "                            stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "    # Split the result into lines (for each GPU if there are multiple)\n",
        "    memory_used_mb_list = result.stdout.strip().splitlines()\n",
        "\n",
        "    # Convert memory used from MB to GB and check each GPU's memory usage\n",
        "    for i, memory_used_mb in enumerate(memory_used_mb_list):\n",
        "        memory_used_gb = int(memory_used_mb) / 1024.0\n",
        "        # print(f\"GPU {i}: Current memory allocated: {memory_used_gb:.2f} GB\")\n",
        "        if memory_used_gb > limit_gb:\n",
        "            # print(f\"GPU {i} memory usage exceeds {limit_gb} GB.\")\n",
        "            return True\n",
        "\n",
        "    # print(\"GPU memory usage is within safe limits.\")\n",
        "    return False\n",
        "\n",
        "def convert_seconds_to_hms(seconds):\n",
        "    hours, remainder = divmod(seconds, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    milliseconds = math.floor((seconds % 1) * 1000)\n",
        "    output = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}\"\n",
        "    return output\n",
        "\n",
        "\n",
        "def load_whisper_turbo_model():\n",
        "  global whisper_turbo_model,base_path\n",
        "  try:\n",
        "    if whisper_turbo_model is not None:\n",
        "      del whisper_turbo_model\n",
        "      whisper_turbo_model=None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(2)\n",
        "  except:\n",
        "      pass\n",
        "  # model_name=\"faster-whisper-large-v3-turbo-ct2\"\n",
        "  model_name=f\"{base_path}/faster-whisper-large-v3-turbo-ct2\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device == \"cuda\":\n",
        "      try:\n",
        "          whisper_turbo_model = WhisperModel(model_name, device=\"cuda\", compute_type=\"float16\")\n",
        "      except Exception as e:\n",
        "          whisper_turbo_model = WhisperModel(model_name, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "  else:\n",
        "      whisper_turbo_model = WhisperModel(model_name, device=\"cpu\", compute_type=\"int8\")\n",
        "  return whisper_turbo_model\n",
        "\n",
        "def subtitle_maker(input_file):\n",
        "  global base_path,whisper_turbo_model\n",
        "  if is_gpu_memory_over_limit(limit_gb=14.5):\n",
        "    whisper_turbo_model=load_whisper_turbo_model()\n",
        "  base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
        "  random_uuid = uuid.uuid4().hex[:4]\n",
        "  subtitle_folder = f\"{base_path}/Generated_Subtitle\"\n",
        "  os.makedirs(subtitle_folder, exist_ok=True)\n",
        "  srt_file_name =f\"{subtitle_folder}/{base_name}.srt\"\n",
        "  srt_file_name=clean_file_name(srt_file_name)\n",
        "  audio_path=get_audio_file(input_file)\n",
        "  segments, info = whisper_turbo_model.transcribe(audio_path, beam_size=5,vad_filter=True,vad_parameters=dict(min_silence_duration_ms=500))\n",
        "  saved_segments = list(segments)\n",
        "  count = 0\n",
        "  sts=\"\"\n",
        "  with open(srt_file_name, 'w',encoding=\"utf-8\") as f:\n",
        "    for i in saved_segments:\n",
        "        segment=list(i)\n",
        "        id=segment[0]\n",
        "        seek=segment[1]\n",
        "        start=segment[2]\n",
        "        end=segment[3]\n",
        "        text=segment[4]\n",
        "        sts+=str(text)\n",
        "        count +=1\n",
        "        duration = f\"{convert_seconds_to_hms(start)} --> {convert_seconds_to_hms(end)}\\n\"\n",
        "        text = f\"{text.lstrip()}\\n\\n\"\n",
        "        f.write(f\"{count}\\n{duration}{text}\")\n",
        "  sts=sts.strip()\n",
        "  text_path=srt_file_name.replace(\".srt\",\".txt\")\n",
        "  with open(text_path, 'w') as file:\n",
        "      file.write(sts)\n",
        "  if os.path.exists(audio_path):\n",
        "    os.remove(audio_path)\n",
        "  return str(srt_file_name),str(text_path),sts\n",
        "base_path=\"/content\"\n",
        "# base_path=\".\"\n",
        "whisper_turbo_model=None\n",
        "whisper_turbo_model=load_whisper_turbo_model()"
      ],
      "metadata": {
        "id": "OM9yRSnjfsws",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio_Or_Video_File_Path = '/content/yt.MP3'  # @param {type: \"string\"}\n",
        "srt_file_name,text_file_name,text=subtitle_maker(Audio_Or_Video_File_Path)\n",
        "print(f\"SRT file save at : {srt_file_name}\")\n",
        "print(f\"TEXT file save at : {text_file_name}\")\n",
        "print(f\"Speech to text save at : 'text' variable \")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(srt_file_name)\n",
        "# files.download(text_file_name)"
      ],
      "metadata": {
        "id": "PthU-L1tkg6A",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4436828c-b374-4523-d0c1-d53852813e31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRT file save at : /content/Generated_Subtitle/yt_ff9754.srt\n",
            "TEXT file save at : /content/Generated_Subtitle/yt_ff9754.txt\n",
            "Speech to text save at : 'text' variable \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3690d09d-0231-45bf-8347-1a1bb729db06\", \"yt_ff9754.srt\", 9898)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(srt_file_name,\"r\",encoding=\"utf-8\") as f:\n",
        "#     srt_data = f.read()\n",
        "# print(srt_data)"
      ],
      "metadata": {
        "id": "sYSfjf1vsrIK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(text_file_name,\"r\",encoding=\"utf-8\") as f:\n",
        "#     text_data = f.read()\n",
        "# print(text_data)"
      ],
      "metadata": {
        "id": "z9lvn9droafO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text"
      ],
      "metadata": {
        "id": "m3lJjPa7vBak"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Using Gradio Interface\n",
        "import gradio as gr\n",
        "# demo_examples = [[\"/content/audio/a.mp3\"]]\n",
        "gradio_inputs=[gr.File(label=\"Upload Audio or Video File\")]\n",
        "gradio_outputs=[gr.File(label=\"Download SRT File\",show_label=True),gr.File(label=\"Download Text File\",show_label=True),gr.Textbox(label=\"Speech To Text\")]\n",
        "demo = gr.Interface(fn=subtitle_maker, inputs=gradio_inputs,outputs=gradio_outputs , title=\"Whisper-Large-V3-Turbo-Ct2 Subtitle Generator\")#,examples=demo_examples)\n",
        "demo.launch(debug=True,share=True)\n"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "1b3OeLSmwA9V",
        "outputId": "ecde5ebc-c334-456e-ca37-ba5a9235a921"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://062848203022ff9412.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://062848203022ff9412.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://062848203022ff9412.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}